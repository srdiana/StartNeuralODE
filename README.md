# Example of using NeuralODE for MNIST dataset

**–ù–µ–π—Ä–æ–Ω–Ω—ã–µ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω—ã–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è (Neural ODE) –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä MNIST —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º adjoint –º–µ—Ç–æ–¥–∞ –∏ –ø–æ–ª–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.**

## –û–±–∑–æ—Ä

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç Neural ODE ‚Äî —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–π –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –¥–∏–Ω–∞–º–∏–∫—É –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π (–û–î–£). –í–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –º—ã –æ–±—É—á–∞–µ–º –û–î–£, –≥–¥–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é.

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏

```
NeuralODEModel(
  (encoder): Sequential(
    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Flatten(start_dim=1, end_dim=-1)
    (7): Linear(in_features=1568, out_features=128, bias=True)
    (8): ReLU()
    (9): Linear(in_features=128, out_features=64, bias=True)
    (10): Tanh()
  )
  (odefunc): ODEFunc(
    (net): Sequential(
      (0): Linear(in_features=64, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=64, bias=True)
    )
  )
  (odeblock): ODEBlock(
    (odefunc): ODEFunc(
      (net): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): ReLU()
        (4): Linear(in_features=128, out_features=128, bias=True)
        (5): ReLU()
        (6): Linear(in_features=128, out_features=64, bias=True)
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=10, bias=True)
  )
)
```


<!-- 
## üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

–ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç 6 —Ç–∏–ø–æ–≤ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: -->
<!-- 
### 1. –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
![Training History](docs/images/training_history.png)
*Loss, accuracy –∏ NFE (Number of Function Evaluations) –ø–æ —ç–ø–æ—Ö–∞–º*

### 2. –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ ODE –≤ PCA –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
![ODE Trajectories](docs/images/ode_trajectories.png)
*–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ü–∏—Ñ—Ä*

### 3. –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (t-SNE)
![Latent Space](docs/images/latent_space.png)
*–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–æ –∏ –ø–æ—Å–ª–µ ODE* -->
<!-- 
### 4. –î–∏–Ω–∞–º–∏–∫–∞ ODE
![ODE Dynamics](docs/images/ode_dynamics.png)
*–≠–≤–æ–ª—é—Ü–∏—è –Ω–æ—Ä–º —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞*

### 5. Confusion Matrix
![Confusion Matrix](docs/images/confusion_matrix.png)
*–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏*

### 6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
![Predictions](docs/images/predictions.png)
*–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏* -->


```python
BATCH_SIZE = 64          # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
EPOCHS = 10              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
LEARNING_RATE = 0.001    # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
T = 1.0                  # –ö–æ–Ω–µ—á–Ω–æ–µ –≤—Ä–µ–º—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è ODE
ODE_DIM = 64             # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
SOLVER = 'dopri5'        # –†–µ—à–∞—Ç–µ–ª—å ODE (dopri5/rk4/euler/midpoint)
RTOL = 1e-3              # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å
ATOL = 1e-4              # –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å
```

### –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—à–∞—Ç–µ–ª–∏ ODE

| –†–µ—à–∞—Ç–µ–ª—å | –¢–æ—á–Ω–æ—Å—Ç—å | –°–∫–æ—Ä–æ—Å—Ç—å | –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å |
|----------|----------|----------|--------------|
| `dopri5` | –í—ã—Å–æ–∫–∞—è | –ú–µ–¥–ª–µ–Ω–Ω–∞—è | –í—ã—Å–æ–∫–∞—è |
| `rk4` | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω—è—è |
| `euler` | –ù–∏–∑–∫–∞—è | –ë—ã—Å—Ç—Ä–∞—è | –ù–∏–∑–∫–∞—è |
| `midpoint` | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω—è—è |

## üß† –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

### Neural ODE –§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

**–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥**:
```
z(t‚ÇÄ) = Encoder(x)
dz/dt = f_Œ∏(z(t), t)  –¥–ª—è t ‚àà [t‚ÇÄ, T]
≈∑ = Classifier(z(T))
```

**–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (Adjoint –º–µ—Ç–æ–¥)**:
```
a(t) = ‚àÇL/‚àÇz(t)  # adjoint —Å–æ—Å—Ç–æ—è–Ω–∏–µ
da/dt = -a(t)·µÄ ‚àÇf_Œ∏/‚àÇz
dL/dŒ∏ = -‚à´‚Çú‚ÇÄ·µÄ a(t)·µÄ ‚àÇf_Œ∏/‚àÇŒ∏ dt
```

1. **Neural Ordinary Differential Equations** (NeurIPS 2018)
   - –ê–≤—Ç–æ—Ä—ã: Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud
   - [arXiv:1806.07366](https://arxiv.org/abs/1806.07366)

2. **FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models** (ICLR 2019)
   - [arXiv:1810.01367](https://arxiv.org/abs/1810.01367)

3. **Latent ODEs for Irregularly-Sampled Time Series** (NeurIPS 2019)
   - [arXiv:1907.03907](https://arxiv.org/abs/1907.03907)

### –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

- **Adjoint Sensitivity Method**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ ODE
- **Continuous Normalizing Flows**: –ù–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–µ –ø–æ—Ç–æ–∫–∏ –∫–∞–∫ ODE
- **Neural Differential Equations**: –û–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ SDE –∏ PDE
